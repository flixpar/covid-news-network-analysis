{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Networks from Articles Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_domain(url):\n",
    "    domain = tldextract.extract(url)\n",
    "    domain = domain.domain + \".\" + domain.suffix\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(url):\n",
    "    url = url.split(\"?\")[0]\n",
    "    url = url.split(\"://\")[-1] if url[:4] == \"http\" else url\n",
    "    if not url: return None\n",
    "    url = url[:-1] if url[-1] == \"/\" else url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parselist(s):\n",
    "    s = s[2:-2]\n",
    "    s = s.split(\"', '\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [x for y in l for x in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data_1 = pd.read_csv(\"../data/articles_collect/aylien_covid_articles_content_selected_1.csv\")\n",
    "articles_data_2 = pd.read_csv(\"../data/articles_collect/aylien_covid_articles_content_selected_2.csv\")\n",
    "articles_data_3 = pd.read_csv(\"../data/articles_collect/coaid_covid_articles_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = pd.concat([articles_data_1, articles_data_2, articles_data_3])\n",
    "articles_data = articles_data.drop_duplicates(subset=[\"url\"])\n",
    "articles_data = articles_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63813"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acs.org', 'androidcentral.com', 'aol.com', 'apa.org', 'apnews.com', 'apple.com', 'aptv.org', 'archdaily.com', 'archive.org', 'arizona.edu', 'avclub.com', 'azcentral.com', 'bgr.com', 'billboard.com', 'bizjournals.com', 'bloomberg.com', 'breitbart.com', 'brobible.com', 'buffalonews.com', 'businessinsider.com', 'bustle.com', 'buzzfeed.com', 'buzzorange.com', 'c-span.org', 'ca.gov', 'cbslocal.com', 'cbsnews.com', 'cbssports.com', 'cdc.gov', 'chicagotribune.com', 'chron.com', 'cisco.com', 'cnbc.com', 'cnet.com', 'cnn.com', 'complex.com', 'cool3c.com', 'cosmopolitan.com', 'cuny.edu', 'dailycaller.com', 'deadline.com', 'delta.com', 'denverpost.com', 'digitaltrends.com', 'ed.gov', 'eonline.com', 'epochtimes.com', 'ew.com', 'fastcompany.com', 'fivethirtyeight.com', 'forbes.com', 'foxbusiness.com', 'foxnews.com', 'go.com', 'google.com', 'gsmarena.com', 'harvard.edu', 'hbr.org', 'healthline.com', 'hindawi.com', 'huffingtonpost.com', 'informationisbeautiful.net', 'intel.com', 'investopedia.com', 'issuu.com', 'jsonline.com', 'latimes.com', 'legacy.com', 'linkedin.com', 'marketwatch.com', 'mashable.com', 'mediaite.com', 'medicalnewstoday.com', 'medium.com', 'menshealth.com', 'mit.edu', 'motorsport.com', 'msn.com', 'msu.edu', 'nabjnahjconvention.com', 'nasa.gov', 'nasdaq.com', 'nature.com', 'nba.com', 'nbcnews.com', 'nbcsports.com', 'newsweek.com', 'newyorker.com', 'nfl.com', 'nih.gov', 'nj.com', 'northwestern.edu', 'npr.org', 'ny.gov', 'nydailynews.com', 'nymag.com', 'nypost.com', 'nysenate.gov', 'nytimes.com', 'oracle.com', 'oreilly.com', 'pbs.org', 'pitchfork.com', 'politico.com', 'princeton.edu', 'producthunt.com', 'psu.edu', 'qz.com', 'radio.com', 'reuters.com', 'rev.com', 'rollingstone.com', 'scientificamerican.com', 'scitation.org', 'sec.gov', 'sfgate.com', 'slate.com', 'snopes.com', 'statesman.com', 'techcrunch.com', 'techtarget.com', 'texas.gov', 'theadvocate.com', 'theatlantic.com', 'thedailybeast.com', 'theguardian.com', 'thehill.com', 'thewheelerreport.com', 'this-is-italy.com', 'time.com', 'tmz.com', 'truththeory.com', 'tutsplus.com', 'twitter.com', 'ucdavis.edu', 'uchicago.edu', 'ucla.edu', 'ufl.edu', 'umich.edu', 'un.org', 'usatoday.com', 'usda.gov', 'vanityfair.com', 'variety.com', 'vice.com', 'virginia.edu', 'vox.com', 'wa.gov', 'weather.gov', 'who.int', 'wired.com', 'yahoo.com', 'yelp.com', 'zdnet.com']\n",
      "n domains: 154\n"
     ]
    }
   ],
   "source": [
    "domains = sorted(list(set(articles_data.domain.tolist())))\n",
    "domains = domains\n",
    "print(domains)\n",
    "print(f\"n domains: {len(domains)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63813/63813 [00:05<00:00, 12579.91it/s]\n"
     ]
    }
   ],
   "source": [
    "articles_per_domain = {d: 0 for d in domains}\n",
    "for i, row in tqdm.tqdm(articles_data.iterrows(), total=len(articles_data)):\n",
    "    articles_per_domain[row.domain] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nolink_ind = [i for i,l in enumerate(articles_data.links) if l == \"[]\"]\n",
    "nolink_domains = articles_data.domain[nolink_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_nolink_bydomain = {d: (nolink_domains.count(d)/articles_per_domain[d]) if articles_per_domain[d] != 0 else 1.0 for d in domains}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apa.org': 1.0,\n",
       " 'bizjournals.com': 1.0,\n",
       " 'bloomberg.com': 0.998109640831758,\n",
       " 'cdc.gov': 0.46037099494097805,\n",
       " 'hindawi.com': 1.0,\n",
       " 'huffingtonpost.com': 1.0,\n",
       " 'latimes.com': 1.0,\n",
       " 'newsweek.com': 1.0,\n",
       " 'nydailynews.com': 0.483974358974359,\n",
       " 'princeton.edu': 1.0,\n",
       " 'rev.com': 0.5,\n",
       " 'texas.gov': 1.0,\n",
       " 'thewheelerreport.com': 1.0,\n",
       " 'weather.gov': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baddomains = {k:v for k,v in proportion_nolink_bydomain.items() if v > 0.4}\n",
    "baddomains_list = list(baddomains.keys())\n",
    "baddomains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = flatten([parselist(l) for l in articles_data.keywords.tolist()])\n",
    "keywords = [k.lower().replace(\"#\", \"\").strip() for k in keywords]\n",
    "keywords_unique = sorted(list(set(keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2628087\n",
      "190385\n"
     ]
    }
   ],
   "source": [
    "print(len(keywords))\n",
    "print(len(keywords_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_counts = {k: keywords.count(k) for k in keywords_unique}\n",
    "# print(keyword_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_important = [\"covid\", \"covid-19\", \"coronavirus\", \"pandemic\", \"cdc\", \"center for disease control\", \"nih\", \"flu\", \"hospital\", \"healthcare\", \"vaccine\", \"mask\", \"lock-down\", \"ventilator\", \"ppe\", \"quarantine\", \"social distancing\", \"social distance\", \"antibody\", \"pcr\", \"epidemic\", \"n95\", \"johns hopkins\", \"csse\", \"fauci\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'covid': 4135, 'covid-19': 30376, 'coronavirus': 62508, 'pandemic': 34300, 'cdc': 3869, 'center for disease control': 0, 'nih': 306, 'flu': 0, 'hospital': 5651, 'healthcare': 2789, 'vaccine': 2982, 'mask': 1326, 'lock-down': 3, 'ventilator': 763, 'ppe': 827, 'quarantine': 6151, 'social distancing': 339, 'social distance': 653, 'antibody': 680, 'pcr': 128, 'epidemic': 2128, 'n95': 0, 'johns hopkins': 8, 'csse': 5, 'fauci': 730}\n"
     ]
    }
   ],
   "source": [
    "keywords_important_counts = {k: keywords.count(k) for k in keywords_important}\n",
    "print(keywords_important_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2964"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.count(\"fox news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_clusters = {\n",
    "    \"sources\": {\"cdc\", \"who\", \"un\", \"fda\", \"nih\", \"nhs\", \"fauci\"},\n",
    "    \"healthcare\": {\"hospital\", \"healthcare\", \"nurses\", \"ventilator\"},\n",
    "    \"ppe\": {\"mask\", \"ppe\"},\n",
    "    \"pandemic-response\": {\"social distance\", \"social distancing\", \"lock-down\", \"quarantine\", \"contact-tracing\"},\n",
    "    \"vaccines\": {\"vaccine\", \"fda\", \"pfizer\", \"moderna\", \"astrazeneca\"},\n",
    "    \"testing\": {\"antibody\", \"pcr\", \"testing\"},\n",
    "    \"jhu\": {\"johns hopkins university\", \"johns hopkins\", \"csse\"},\n",
    "    \"economy\": {\"stimulus\", \"relief\", \"economy\", \"jobs\"},\n",
    "    \"metrics\": {\"cases\", \"deaths\"},\n",
    "    \"symptoms\": {\"symptoms\"},\n",
    "    \"trump\": {\"trump\"},\n",
    "    \"china\": {\"china\", \"wuhan\"},\n",
    "    \"conspiracy-theories\": {\"hydroxychloroquine\", \"bleach\", \"herd immunity\", \"conspiracy\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_keywords = {\n",
    "    \"cdc\": \"cdc.gov\",\n",
    "    \"nytimes\": \"nytimes.com\", \"new york times\": \"nytimes.com\", \"the times\": \"nytimes.com\",\n",
    "    \"fox news\": \"foxnews.com\",\n",
    "    \"cnn\": \"cnn.com\",\n",
    "    \"nbc\": \"nbcnews.com\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63813/63813 [02:36<00:00, 408.74it/s]\n"
     ]
    }
   ],
   "source": [
    "articles_data[\"source_links\"] = [\"[]\" for i in range(len(articles_data))]\n",
    "for i,row in tqdm.tqdm(articles_data.iterrows(), total=len(articles_data)):\n",
    "    f = []\n",
    "    for k in source_keywords.keys():\n",
    "        try:\n",
    "            if row.content.find(k) > 0:\n",
    "                f.append(k)\n",
    "        except:\n",
    "            continue\n",
    "    articles_data.loc[i, \"source_links\"] = str(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_articlelevel_directlinks():\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    article_urls = [normalize_url(l) for l in articles_data.url.tolist()]\n",
    "    article_domains = articles_data.domain.tolist()\n",
    "    for i in range(len(article_urls)):\n",
    "        G.add_node(article_urls[i], domain=article_domains[i])\n",
    "    \n",
    "    article_urls_set = set(article_urls)\n",
    "    for i, row in tqdm.tqdm(articles_data.iterrows(), total=len(articles_data)):\n",
    "        url = normalize_url(row.url)\n",
    "        for l in parselist(row.links):\n",
    "            l = normalize_url(l)\n",
    "            if l is None: continue\n",
    "            if l in article_urls_set:\n",
    "                G.add_edge(url, l)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_articlelevel_alllinks():\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    article_urls = [normalize_url(l) for l in articles_data.url.tolist()]\n",
    "    article_domains = articles_data.domain.tolist()\n",
    "    for i in range(len(article_urls)):\n",
    "        G.add_node(article_urls[i], domain=article_domains[i])\n",
    "    \n",
    "    article_urls_set = set(article_urls)\n",
    "    for i, row in tqdm.tqdm(articles_data.iterrows(), total=len(articles_data)):\n",
    "        url = normalize_url(row.url)\n",
    "        for l in parselist(row.links):\n",
    "            l = normalize_url(l)\n",
    "            if l is None: continue\n",
    "            if l in article_urls_set:\n",
    "                G.add_edge(url, l)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_articlelevel_keywordclusters():\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    article_urls = [normalize_url(l) for l in articles_data.url.tolist()]\n",
    "    article_domains = articles_data.domain.tolist()\n",
    "    for i in range(len(article_urls)):\n",
    "        G.add_node(article_urls[i], domain=article_domains[i])\n",
    "    \n",
    "    article_urls_set = set(article_urls)\n",
    "    for i, row in tqdm.tqdm(articles_data.iterrows(), total=len(articles_data)):\n",
    "        url = normalize_url(row.url)\n",
    "        for l in parselist(row.links):\n",
    "            l = normalize_url(l)\n",
    "            if l is None: continue\n",
    "            if l in article_urls_set:\n",
    "                G.add_edge(url, l)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63813/63813 [00:07<00:00, 8191.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V|=63514, |E|=9789\n"
     ]
    }
   ],
   "source": [
    "graph = generate_network_articlelevel_directlinks()\n",
    "print(f\"|V|={graph.number_of_nodes()}, |E|={graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63813/63813 [00:08<00:00, 7486.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V|=63514, |E|=9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph = generate_network_articlelevel_alllinks()\n",
    "print(f\"|V|={graph.number_of_nodes()}, |E|={graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
